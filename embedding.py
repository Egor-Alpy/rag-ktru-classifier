import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
import logging
from config import EMBEDDING_MODEL, VECTOR_SIZE

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class EmbeddingModel:
    def __init__(self, model_name=EMBEDDING_MODEL):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()  # –†–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.model = self.model.to(self.device)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        self._determine_model_dimension()

        logger.info(f"–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        logger.info(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {self.model_dimension}")

    def _determine_model_dimension(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"""
        try:
            # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            test_text = "—Ç–µ—Å—Ç"
            inputs = self.tokenizer(test_text, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º CLS —Ç–æ–∫–µ–Ω
            test_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

            if test_embedding.ndim > 0:
                self.model_dimension = len(test_embedding)
            else:
                self.model_dimension = test_embedding.shape[0] if hasattr(test_embedding, 'shape') else VECTOR_SIZE

            logger.info(f"‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {self.model_dimension}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {e}")
            self.model_dimension = VECTOR_SIZE

    def generate_embedding(self, text):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if not text or text.strip() == "":
            logger.warning("–ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞")
            return np.zeros(self.model_dimension)

        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            inputs = self.tokenizer(text,
                                    return_tensors="pt",
                                    padding=True,
                                    truncation=True,
                                    max_length=512)

            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            with torch.no_grad():
                outputs = self.model(**inputs)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º CLS —Ç–æ–∫–µ–Ω –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤, –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å > 1
            if embedding.ndim > 1 and embedding.shape[0] == 1:
                embedding = embedding[0]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            if len(embedding) != self.model_dimension:
                logger.warning(
                    f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(embedding)}, –æ–∂–∏–¥–∞–ª–∞—Å—å: {self.model_dimension}")

                # –ü—Ä–æ—Å—Ç–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                if len(embedding) > self.model_dimension:
                    embedding = embedding[:self.model_dimension]
                else:
                    # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                    padding = np.zeros(self.model_dimension - len(embedding))
                    embedding = np.concatenate([embedding, padding])

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            else:
                logger.warning("–ü–æ–ª—É—á–µ–Ω –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")

            return embedding

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return np.zeros(self.model_dimension)

    def generate_batch_embeddings(self, texts, batch_size=32):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–∞–∫–µ—Ç–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        embeddings = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç–∞–º–∏
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = []

            for text in batch_texts:
                embedding = self.generate_embedding(text)
                batch_embeddings.append(embedding)

            embeddings.extend(batch_embeddings)

            if i % (batch_size * 10) == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 10 –ø–∞–∫–µ—Ç–æ–≤
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(embeddings)}/{len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")

        return embeddings

    def test_embedding_quality(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

        test_cases = [
            "–Ω–æ—É—Ç–±—É–∫ –∫–æ–º–ø—å—é—Ç–µ—Ä",
            "—Ä—É—á–∫–∞ –∫–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã",
            "—Å—Ç–æ–ª –º–µ–±–µ–ª—å –æ—Ñ–∏—Å–Ω–∞—è",
            "–±—É–º–∞–≥–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–ª—å–Ω–∞—è",
            "–ø—Ä–∏–Ω—Ç–µ—Ä –æ—Ä–≥—Ç–µ—Ö–Ω–∏–∫–∞"
        ]

        embeddings = []
        for text in test_cases:
            emb = self.generate_embedding(text)
            embeddings.append(emb)
            logger.info(f"   '{text}' -> —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(emb)}, –Ω–æ—Ä–º–∞: {np.linalg.norm(emb):.3f}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏
        logger.info("üîç –°—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É —Ç–µ—Å—Ç–æ–≤—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏:")
        for i in range(len(embeddings)):
            for j in range(i + 1, len(embeddings)):
                similarity = np.dot(embeddings[i], embeddings[j])
                logger.info(f"   '{test_cases[i]}' <-> '{test_cases[j]}': {similarity:.3f}")


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
embedding_model = EmbeddingModel()


def generate_embedding(text):
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
    return embedding_model.generate_embedding(text)


def generate_batch_embeddings(texts, batch_size=32):
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞–∫–µ—Ç–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    return embedding_model.generate_batch_embeddings(texts, batch_size)


def test_embeddings():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    embedding_model.test_embedding_quality()


# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ (—Ç–æ–ª—å–∫–æ –≤ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ)
if __name__ == "__main__":
    test_embeddings()