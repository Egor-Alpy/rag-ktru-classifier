import re
import torch
import json
import logging
from typing import List, Dict, Optional, Tuple, Set
from collections import defaultdict
import numpy as np
from difflib import SequenceMatcher
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer, GenerationConfig, LlamaTokenizer
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue, MatchText
from embedding import generate_embedding
from config import (
    LLM_BASE_MODEL, LLM_ADAPTER_MODEL, QDRANT_HOST, QDRANT_PORT,
    QDRANT_COLLECTION, TEMPERATURE, TOP_P, REPETITION_PENALTY,
    MAX_NEW_TOKENS, TOP_K
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class UtilityKtruClassifier:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Ç–∏–ª–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ö–¢–†–£"""
        self.qdrant_client = self._setup_qdrant()
        self.llm, self.tokenizer = self._setup_llm()

        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ö–¢–†–£ –∫–æ–¥–∞
        self.ktru_pattern = re.compile(r'\d{2}\.\d{2}\.\d{2}\.\d{3}-\d{8}')

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.keyword_index = self._build_keyword_index()

        # –ö–∞—Ä—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–≤–∞—Ä–æ–≤ –∏ –∏—Ö —Ç–∏–ø–∏—á–Ω—ã—Ö –∫–æ–¥–æ–≤
        self.category_patterns = {
            # –ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞
            '–∫–æ–º–ø—å—é—Ç–µ—Ä': {'codes': ['26.20.11', '26.20.13', '26.20.14'], 'weight': 1.0},
            '–Ω–æ—É—Ç–±—É–∫': {'codes': ['26.20.11', '26.20.13'], 'weight': 1.0},
            'laptop': {'codes': ['26.20.11', '26.20.13'], 'weight': 1.0},
            '–ø–∫': {'codes': ['26.20.11', '26.20.13'], 'weight': 0.9},
            '—Å–∏—Å—Ç–µ–º–Ω—ã–π –±–ª–æ–∫': {'codes': ['26.20.11'], 'weight': 1.0},
            '–º–æ–Ω–∏—Ç–æ—Ä': {'codes': ['26.20.17'], 'weight': 1.0},
            '–∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞': {'codes': ['26.20.16'], 'weight': 1.0},
            '–º—ã—à—å': {'codes': ['26.20.16'], 'weight': 1.0},
            '–ø—Ä–∏–Ω—Ç–µ—Ä': {'codes': ['26.20.16', '30.20'], 'weight': 1.0},
            '—Å–∫–∞–Ω–µ—Ä': {'codes': ['26.20.16'], 'weight': 1.0},
            '–º—Ñ—É': {'codes': ['26.20.16'], 'weight': 1.0},

            # –ö–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã
            '—Ä—É—á–∫–∞': {'codes': ['32.99.12', '32.99.13'], 'weight': 1.0},
            '–∫–∞—Ä–∞–Ω–¥–∞—à': {'codes': ['32.99.15'], 'weight': 1.0},
            '–º–∞—Ä–∫–µ—Ä': {'codes': ['32.99.12'], 'weight': 1.0},
            '—Ñ–ª–æ–º–∞—Å—Ç–µ—Ä': {'codes': ['32.99.12'], 'weight': 1.0},
            '–ª–∞—Å—Ç–∏–∫': {'codes': ['22.19.71'], 'weight': 1.0},
            '–ª–∏–Ω–µ–π–∫–∞': {'codes': ['32.99.15'], 'weight': 1.0},
            '—Ç–µ—Ç—Ä–∞–¥—å': {'codes': ['17.23.13'], 'weight': 1.0},
            '–±–ª–æ–∫–Ω–æ—Ç': {'codes': ['17.23.13'], 'weight': 1.0},
            '—Å—Ç–µ–ø–ª–µ—Ä': {'codes': ['25.99.23'], 'weight': 1.0},
            '—Å–∫—Ä–µ–ø–∫–∏': {'codes': ['25.93.18'], 'weight': 1.0},
            '—Å–∫–æ—Ç—á': {'codes': ['22.21.21'], 'weight': 1.0},
            '–∫–ª–µ–π': {'codes': ['20.52'], 'weight': 1.0},

            # –ë—É–º–∞–∂–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è
            '–±—É–º–∞–≥–∞': {'codes': ['17.12.14', '17.23.12'], 'weight': 1.0},
            '—Ç—É–∞–ª–µ—Ç–Ω–∞—è –±—É–º–∞–≥–∞': {'codes': ['17.22.12'], 'weight': 1.0},
            '—Å–∞–ª—Ñ–µ—Ç–∫–∏': {'codes': ['17.22.13'], 'weight': 1.0},
            '–ø–æ–ª–æ—Ç–µ–Ω—Ü–∞ –±—É–º–∞–∂–Ω—ã–µ': {'codes': ['17.22.13'], 'weight': 1.0},
            '–∫–∞—Ä—Ç–æ–Ω': {'codes': ['17.12.42'], 'weight': 1.0},

            # –ú–µ–±–µ–ª—å
            '—Å—Ç–æ–ª': {'codes': ['31.01.11', '31.09.11'], 'weight': 1.0},
            '—Å—Ç—É–ª': {'codes': ['31.01.11', '31.09.12'], 'weight': 1.0},
            '–∫—Ä–µ—Å–ª–æ': {'codes': ['31.01.12', '31.09.12'], 'weight': 1.0},
            '—à–∫–∞—Ñ': {'codes': ['31.01.13', '31.09.13'], 'weight': 1.0},
            '—Ç—É–º–±–∞': {'codes': ['31.09.13'], 'weight': 1.0},
            '–ø–æ–ª–∫–∞': {'codes': ['31.09.13'], 'weight': 1.0},
            '–¥–∏–≤–∞–Ω': {'codes': ['31.09.11'], 'weight': 1.0},

            # –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã
            '—à–ø—Ä–∏—Ü': {'codes': ['32.50.13'], 'weight': 1.0},
            '–±–∏–Ω—Ç': {'codes': ['21.20.24'], 'weight': 1.0},
            '–º–∞—Å–∫–∞': {'codes': ['32.50.22', '14.12.30'], 'weight': 1.0},
            '–ø–µ—Ä—á–∞—Ç–∫–∏': {'codes': ['22.19.60', '15.20.32'], 'weight': 1.0},
            '—Ç–µ—Ä–º–æ–º–µ—Ç—Ä': {'codes': ['26.51.53'], 'weight': 1.0},
            '—Ç–æ–Ω–æ–º–µ—Ç—Ä': {'codes': ['26.60.12'], 'weight': 1.0},

            # –ü—Ä–æ–¥—É–∫—Ç—ã –ø–∏—Ç–∞–Ω–∏—è
            '–º–æ–ª–æ–∫–æ': {'codes': ['10.51.11', '10.51.12'], 'weight': 1.0},
            '—Ö–ª–µ–±': {'codes': ['10.71.11'], 'weight': 1.0},
            '–º—è—Å–æ': {'codes': ['10.11', '10.13'], 'weight': 1.0},
            '—Ä—ã–±–∞': {'codes': ['10.20'], 'weight': 1.0},
            '–æ–≤–æ—â–∏': {'codes': ['01.13'], 'weight': 1.0},
            '—Ñ—Ä—É–∫—Ç—ã': {'codes': ['01.24', '01.25'], 'weight': 1.0},
        }

        # –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        self.synonyms = {
            '–Ω–æ—É—Ç–±—É–∫': ['–ª—ç–ø—Ç–æ–ø', '–ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä', '–Ω–æ—É—Ç', 'notebook', 'laptop'],
            '–∫–æ–º–ø—å—é—Ç–µ—Ä': ['–ø–∫', '–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä', '–∫–æ–º–ø', '—Å–∏—Å—Ç–µ–º–Ω—ã–π –±–ª–æ–∫', '–¥–µ—Å–∫—Ç–æ–ø'],
            '—Ä—É—á–∫–∞': ['–∞–≤—Ç–æ—Ä—É—á–∫–∞', '—à–∞—Ä–∏–∫–æ–≤–∞—è —Ä—É—á–∫–∞', '–≥–µ–ª–µ–≤–∞—è —Ä—É—á–∫–∞', '—Ä—É—á–∫–∞ –¥–ª—è –ø–∏—Å—å–º–∞'],
            '–±—É–º–∞–≥–∞': ['–ª–∏—Å—Ç—ã', '–±—É–º–∞–∂–Ω—ã–µ –ª–∏—Å—Ç—ã', '–æ—Ñ–∏—Å–Ω–∞—è –±—É–º–∞–≥–∞', '–ø–∏—Å—á–∞—è –±—É–º–∞–≥–∞'],
            '—Å—Ç–æ–ª': ['–ø–∏—Å—å–º–µ–Ω–Ω—ã–π —Å—Ç–æ–ª', '—Ä–∞–±–æ—á–∏–π —Å—Ç–æ–ª', '–æ—Ñ–∏—Å–Ω—ã–π —Å—Ç–æ–ª', '–ø–∞—Ä—Ç–∞'],
            '–ø—Ä–∏–Ω—Ç–µ—Ä': ['–ø–µ—á–∞—Ç–∞—é—â–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ', '–ª–∞–∑–µ—Ä–Ω—ã–π –ø—Ä–∏–Ω—Ç–µ—Ä', '—Å—Ç—Ä—É–π–Ω—ã–π –ø—Ä–∏–Ω—Ç–µ—Ä'],
        }

    def _setup_qdrant(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant"""
        try:
            qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
            collections = qdrant_client.get_collections()
            logger.info(f"‚úÖ Qdrant –ø–æ–¥–∫–ª—é—á–µ–Ω, –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections.collections)}")
            return qdrant_client
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant: {e}")
            return None

    def _setup_llm(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ LLM –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
            tokenizer = AutoTokenizer.from_pretrained(LLM_BASE_MODEL, trust_remote_code=True)

            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            try:
                model = AutoPeftModelForCausalLM.from_pretrained(
                    LLM_ADAPTER_MODEL,
                    device_map="auto",
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                )
                logger.info("‚úÖ LLM –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except:
                logger.warning("‚ö†Ô∏è LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ rule-based –ø–æ–¥—Ö–æ–¥")
                return None, None

            return model, tokenizer
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LLM: {e}")
            return None, None

    def _build_keyword_index(self):
        """–°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –±–∞–∑—ã –ö–¢–†–£"""
        keyword_index = defaultdict(list)

        if not self.qdrant_client:
            return keyword_index

        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞
            offset = None
            batch_size = 100

            while True:
                records, offset = self.qdrant_client.scroll(
                    collection_name=QDRANT_COLLECTION,
                    limit=batch_size,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False
                )

                if not records:
                    break

                for record in records:
                    payload = record.payload
                    ktru_code = payload.get('ktru_code', '')
                    title = payload.get('title', '').lower()

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
                    words = re.findall(r'\b[–∞-—è—ë]+\b', title, re.IGNORECASE)
                    for word in words:
                        if len(word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                            keyword_index[word].append({
                                'code': ktru_code,
                                'title': payload.get('title', ''),
                                'full_data': payload
                            })

                if offset is None:
                    break

            logger.info(f"‚úÖ –ü–æ—Å—Ç—Ä–æ–µ–Ω –∏–Ω–¥–µ–∫—Å –∏–∑ {len(keyword_index)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")

        return keyword_index

    def _extract_keywords(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()

        # –ò—â–µ–º –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        keywords = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for pattern, info in self.category_patterns.items():
            if pattern in text_lower:
                keywords.append((pattern, info['weight']))

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
        for main_word, synonyms in self.synonyms.items():
            if main_word in text_lower:
                keywords.append((main_word, 1.0))
            for synonym in synonyms:
                if synonym in text_lower:
                    keywords.append((main_word, 0.9))  # –°–∏–Ω–æ–Ω–∏–º—ã –∏–º–µ—é—Ç —á—É—Ç—å –º–µ–Ω—å—à–∏–π –≤–µ—Å

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', text_lower, re.IGNORECASE)
        for word in words:
            if word not in [kw[0] for kw in keywords]:
                keywords.append((word, 0.5))

        return keywords

    def _search_by_keywords(self, keywords, limit=20):
        """–ü–æ–∏—Å–∫ –ö–¢–†–£ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        candidates = defaultdict(float)

        for keyword, weight in keywords:
            # –ò—â–µ–º –≤ –∏–Ω–¥–µ–∫—Å–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            if keyword in self.keyword_index:
                for item in self.keyword_index[keyword]:
                    candidates[item['code']] += weight

            # –ò—â–µ–º –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            if keyword in self.category_patterns:
                pattern_info = self.category_patterns[keyword]
                for code_prefix in pattern_info['codes']:
                    # –ò—â–µ–º –≤—Å–µ –∫–æ–¥—ã, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å —ç—Ç–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞
                    for kw_items in self.keyword_index.values():
                        for item in kw_items:
                            if item['code'].startswith(code_prefix):
                                candidates[item['code']] += pattern_info[
                                                                'weight'] * 2  # –£–¥–≤–∞–∏–≤–∞–µ–º –≤–µ—Å –¥–ª—è —Ç–æ—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Å—É
        sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–ø –∫–∞–Ω–¥–∏–¥–∞—Ç–∞—Ö
        result = []
        for code, score in sorted_candidates[:limit]:
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–¥–µ
            for kw_items in self.keyword_index.values():
                for item in kw_items:
                    if item['code'] == code:
                        result.append({
                            'code': code,
                            'score': score,
                            'data': item['full_data']
                        })
                        break
                if len(result) > len([r for r in result if r['code'] != code]):
                    break

        return result

    def _calculate_title_similarity(self, sku_title, ktru_title):
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–π"""
        sku_words = set(re.findall(r'\b[–∞-—è—ëa-z]+\b', sku_title.lower(), re.IGNORECASE))
        ktru_words = set(re.findall(r'\b[–∞-—è—ëa-z]+\b', ktru_title.lower(), re.IGNORECASE))

        if not sku_words or not ktru_words:
            return 0.0

        # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤
        common_words = sku_words.intersection(ktru_words)
        word_similarity = len(common_words) / min(len(sku_words), len(ktru_words))

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏–º–≤–æ–ª–æ–≤
        sequence_similarity = SequenceMatcher(None, sku_title.lower(), ktru_title.lower()).ratio()

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        return word_similarity * 0.7 + sequence_similarity * 0.3

    def _hybrid_search(self, sku_data, top_k=50):
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ + –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫"""
        title = sku_data.get('title', '')
        description = sku_data.get('description', '')

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self._extract_keywords(f"{title} {description}")

        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keyword_results = self._search_by_keywords(keywords, limit=30)

        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        search_text = f"{title} {description}"
        if sku_data.get('category'):
            search_text += f" –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {sku_data['category']}"
        if sku_data.get('brand'):
            search_text += f" –±—Ä–µ–Ω–¥: {sku_data['brand']}"

        embedding = generate_embedding(search_text)

        vector_results = []
        if self.qdrant_client:
            try:
                search_result = self.qdrant_client.search(
                    collection_name=QDRANT_COLLECTION,
                    query_vector=embedding.tolist(),
                    limit=top_k
                )

                for result in search_result:
                    vector_results.append({
                        'code': result.payload.get('ktru_code', ''),
                        'score': getattr(result, 'score', 0),
                        'data': result.payload
                    })
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_results = {}

        # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–æ–Ω–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ)
        for item in keyword_results:
            code = item['code']
            all_results[code] = {
                'code': code,
                'keyword_score': item['score'],
                'vector_score': 0,
                'data': item['data']
            }

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for item in vector_results:
            code = item['code']
            if code in all_results:
                all_results[code]['vector_score'] = item['score']
            else:
                all_results[code] = {
                    'code': code,
                    'keyword_score': 0,
                    'vector_score': item['score'],
                    'data': item['data']
                }

        # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
        final_results = []
        for code, scores in all_results.items():
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç–¥–∞–µ–º –ø–æ–∏—Å–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            final_score = scores['keyword_score'] * 0.7 + scores['vector_score'] * 0.3

            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π
            ktru_title = scores['data'].get('title', '')
            title_similarity = self._calculate_title_similarity(title, ktru_title)
            final_score += title_similarity * 0.5

            final_results.append({
                'code': code,
                'score': final_score,
                'data': scores['data'],
                'keyword_score': scores['keyword_score'],
                'vector_score': scores['vector_score'],
                'title_similarity': title_similarity
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Å–∫–æ—Ä—É
        final_results.sort(key=lambda x: x['score'], reverse=True)

        return final_results[:top_k]

    def classify_sku(self, sku_data, top_k=TOP_K):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è SKU –ø–æ –ö–¢–†–£ –∫–æ–¥—É"""
        logger.info(f"üöÄ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {sku_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")

        if not self.qdrant_client:
            logger.error("‚ùå Qdrant –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

        try:
            # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
            search_results = self._hybrid_search(sku_data, top_k=top_k)

            if not search_results:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ö–¢–†–£ –∫–æ–¥–æ–≤")
                return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            logger.info(f"üìä –¢–æ–ø-5 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤:")
            for i, result in enumerate(search_results[:5]):
                logger.info(f"   {i + 1}. {result['code']} | –°–∫–æ—Ä: {result['score']:.3f} | "
                            f"KW: {result['keyword_score']:.2f} | Vec: {result['vector_score']:.2f} | "
                            f"Title: {result['title_similarity']:.2f} | {result['data'].get('title', '')[:50]}...")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            best_match = search_results[0]

            # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–π –ø–æ–±–µ–¥–∏—Ç–µ–ª—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            if best_match['keyword_score'] > 2.0 and best_match['title_similarity'] > 0.5:
                confidence = min(0.98, 0.7 + best_match['title_similarity'] * 0.3)
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence:.3f}")
                return {
                    "ktru_code": best_match['code'],
                    "ktru_title": best_match['data'].get('title', None),
                    "confidence": confidence
                }

            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
            if best_match['title_similarity'] > 0.7:
                confidence = min(0.95, 0.6 + best_match['title_similarity'] * 0.35)
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence:.3f}")
                return {
                    "ktru_code": best_match['code'],
                    "ktru_title": best_match['data'].get('title', None),
                    "confidence": confidence
                }

            # –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å LLM
            if self.llm and self.tokenizer and len(search_results) > 1:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
                logger.info("ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è...")

                prompt = self._create_simple_prompt(sku_data, search_results[:5])

                try:
                    inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
                    inputs = {k: v.to(self.llm.device) for k, v in inputs.items()}

                    generation_config = GenerationConfig(
                        temperature=0.1,
                        top_p=0.9,
                        max_new_tokens=50,
                        do_sample=True,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )

                    with torch.no_grad():
                        generated_ids = self.llm.generate(**inputs, generation_config=generation_config)

                    response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                    response = response[len(prompt):].strip()

                    logger.info(f"ü§ñ –û—Ç–≤–µ—Ç LLM: '{response}'")

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–≤–µ—Ç
                    ktru_match = self.ktru_pattern.search(response)
                    if ktru_match:
                        ktru_code = ktru_match.group(0)
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–¥ –µ—Å—Ç—å –≤ –Ω–∞—à–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–∞—Ö
                        for result in search_results[:5]:
                            if result['code'] == ktru_code:
                                return {
                                    "ktru_code": ktru_code,
                                    "ktru_title": result['data'].get('title', None),
                                    "confidence": 0.90
                                }
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ LLM: {e}")

            # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞, –Ω–æ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if best_match['score'] > 1.0:
                confidence = min(0.85, 0.5 + best_match['score'] * 0.1)
                logger.info(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence:.3f}")
                return {
                    "ktru_code": best_match['code'],
                    "ktru_title": best_match['data'].get('title', None),
                    "confidence": confidence
                }

            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç
            logger.info("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –ö–¢–†–£ –∫–æ–¥–∞")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

    def _create_simple_prompt(self, sku_data, candidates):
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM"""
        prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏ –ö–¢–†–£ –∫–æ–¥ –¥–ª—è —Ç–æ–≤–∞—Ä–∞.

–¢–û–í–ê–†: {sku_data.get('title', '')}
{f"–û–ø–∏—Å–∞–Ω–∏–µ: {sku_data.get('description', '')}" if sku_data.get('description') else ""}

–ö–ê–ù–î–ò–î–ê–¢–´ –ö–¢–†–£:
"""
        for i, candidate in enumerate(candidates, 1):
            prompt += f"{i}. {candidate['code']} - {candidate['data'].get('title', '')}\n"

        prompt += "\n–í—ã–±–µ—Ä–∏ –ù–ê–ò–ë–û–õ–ï–ï –ü–û–î–•–û–î–Ø–©–ò–ô –∫–æ–¥ –∏–∑ —Å–ø–∏—Å–∫–∞ –≤—ã—à–µ. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ–¥–æ–º:"

        return prompt


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
classifier = UtilityKtruClassifier()


def classify_sku(sku_data: Dict, top_k: int = TOP_K) -> Dict:
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ SKU"""
    return classifier.classify_sku(sku_data, top_k)