import re
import torch
import json
import logging
from typing import List, Dict, Optional, Tuple
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer, GenerationConfig, LlamaTokenizer
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
from embedding import generate_embedding
from config import (
    LLM_BASE_MODEL, LLM_ADAPTER_MODEL, QDRANT_HOST, QDRANT_PORT,
    QDRANT_COLLECTION, TEMPERATURE, TOP_P, REPETITION_PENALTY,
    MAX_NEW_TOKENS, TOP_K, SIMILARITY_THRESHOLD_HIGH,
    SIMILARITY_THRESHOLD_MEDIUM, SIMILARITY_THRESHOLD_LOW,
    CLASSIFICATION_CONFIDENCE_THRESHOLD, ENABLE_ATTRIBUTE_MATCHING,
    ATTRIBUTE_WEIGHT
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class KtruClassifier:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ö–¢–†–£"""
        self.qdrant_client = self._setup_qdrant()
        self.llm, self.tokenizer = self._setup_llm()

        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ö–¢–†–£ –∫–æ–¥–∞
        self.ktru_pattern = re.compile(r'\d{2}\.\d{2}\.\d{2}\.\d{3}-\d{8}')

        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        self.attribute_normalization = {
            "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤": ["—Å–ª–æ–π–Ω–æ—Å—Ç—å", "—á–∏—Å–ª–æ —Å–ª–æ–µ–≤", "—Å–ª–æ–∏"],
            "—Ü–≤–µ—Ç": ["–æ–∫—Ä–∞—Å–∫–∞", "—Ä–∞—Å—Ü–≤–µ—Ç–∫–∞", "–æ—Ç—Ç–µ–Ω–æ–∫"],
            "—Ç–∏–ø": ["–≤–∏–¥", "—Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç—å", "–∫–∞—Ç–µ–≥–æ—Ä–∏—è"],
            "–º–∞—Ç–µ—Ä–∏–∞–ª": ["—Å–æ—Å—Ç–∞–≤", "—Å—ã—Ä—å–µ", "–æ—Å–Ω–æ–≤–∞"],
            "—Ä–∞–∑–º–µ—Ä": ["–≥–∞–±–∞—Ä–∏—Ç", "–≤–µ–ª–∏—á–∏–Ω–∞", "—Ñ–æ—Ä–º–∞—Ç"],
            "–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ": ["–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ", "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ", "—Ü–µ–ª—å"]
        }

    def _setup_qdrant(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant"""
        try:
            qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
            collections = qdrant_client.get_collections()
            logger.info(f"‚úÖ Qdrant –ø–æ–¥–∫–ª—é—á–µ–Ω, –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections.collections)}")
            return qdrant_client
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant: {e}")
            return None

    def _setup_llm(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {LLM_BASE_MODEL}")

            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞
            tokenizer = None
            model = None

            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    LLM_BASE_MODEL,
                    trust_remote_code=True,
                    use_fast=False
                )
                logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞: {e}")
                try:
                    tokenizer = LlamaTokenizer.from_pretrained(
                        LLM_BASE_MODEL,
                        trust_remote_code=True
                    )
                    logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (LlamaTokenizer)")
                except Exception as e2:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä: {e2}")
                    return None, None

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            if tokenizer.pad_token_id is None:
                tokenizer.pad_token_id = tokenizer.eos_token_id

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
            device_available = torch.cuda.is_available()
            logger.info(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {device_available}")

            if device_available:
                try:
                    test_tensor = torch.tensor([1.0], dtype=torch.bfloat16, device='cuda')
                    torch_dtype = torch.bfloat16
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è bfloat16")
                except:
                    torch_dtype = torch.float16
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è float16")
            else:
                torch_dtype = torch.float32
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è float32 (CPU —Ä–µ–∂–∏–º)")

            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            try:
                model = AutoPeftModelForCausalLM.from_pretrained(
                    LLM_ADAPTER_MODEL,
                    device_map="auto",
                    torch_dtype=torch_dtype
                )
                logger.info("‚úÖ PEFT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PEFT –º–æ–¥–µ–ª–∏: {e}")
                try:
                    from transformers import AutoModelForCausalLM
                    model = AutoModelForCausalLM.from_pretrained(
                        LLM_BASE_MODEL,
                        device_map="auto",
                        torch_dtype=torch_dtype
                    )
                    logger.info("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                except Exception as e2:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e2}")
                    return None, None

            return model, tokenizer

        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return None, None

    def _normalize_attribute_name(self, attr_name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–∞"""
        attr_lower = attr_name.lower().strip()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–æ–≤–∞—Ä—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        for normalized, variants in self.attribute_normalization.items():
            if attr_lower == normalized or attr_lower in variants:
                return normalized

        return attr_lower

    def _extract_attributes(self, data: Dict) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤"""
        attributes = {}

        if 'attributes' in data and isinstance(data['attributes'], list):
            for attr in data['attributes']:
                if isinstance(attr, dict):
                    # –î–ª—è SKU —Ñ–æ—Ä–º–∞—Ç
                    if 'attr_name' in attr and 'attr_value' in attr:
                        name = self._normalize_attribute_name(attr['attr_name'])
                        attributes[name] = str(attr['attr_value']).lower()
                    # –î–ª—è KTRU —Ñ–æ—Ä–º–∞—Ç
                    elif 'attr_name' in attr and 'attr_values' in attr:
                        name = self._normalize_attribute_name(attr['attr_name'])
                        values = []
                        for val in attr['attr_values']:
                            if isinstance(val, dict) and 'value' in val:
                                values.append(str(val['value']).lower())
                        if values:
                            attributes[name] = '; '.join(values)

        return attributes

    def _calculate_attribute_similarity(self, sku_attrs: Dict[str, str], ktru_attrs: Dict[str, str]) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤"""
        if not sku_attrs or not ktru_attrs:
            return 0.0

        matches = 0
        total_comparisons = 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        for sku_attr_name, sku_attr_value in sku_attrs.items():
            if sku_attr_name in ktru_attrs:
                ktru_value = ktru_attrs[sku_attr_name]
                total_comparisons += 1

                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if sku_attr_value == ktru_value:
                    matches += 1
                # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                elif sku_attr_value in ktru_value or ktru_value in sku_attr_value:
                    matches += 0.5
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (–¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
                elif ';' in ktru_value:
                    ktru_values = [v.strip() for v in ktru_value.split(';')]
                    if any(sku_attr_value in v or v in sku_attr_value for v in ktru_values):
                        matches += 0.5

        if total_comparisons == 0:
            return 0.0

        return matches / total_comparisons

    def _create_advanced_prompt(self, sku_data: Dict, similar_ktru_entries: List,
                                sku_attrs: Dict[str, str]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ –ö–¢–†–£ (–ö–∞—Ç–∞–ª–æ–≥ —Ç–æ–≤–∞—Ä–æ–≤, —Ä–∞–±–æ—Ç, —É—Å–ª—É–≥).
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –Ω–∞–π—Ç–∏ –ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –¢–û–ß–ù–´–ô –∫–æ–¥ –ö–¢–†–£ –¥–ª—è —Ç–æ–≤–∞—Ä–∞ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –Ω–µ –º–µ–Ω–µ–µ 95%.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –ö–æ–¥ –ö–¢–†–£ –¥–æ–ª–∂–µ–Ω –¢–û–ß–ù–û —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ç–∏–ø—É —Ç–æ–≤–∞—Ä–∞, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –±—ã—Ç—å –ø–æ—Ö–æ–∂–∏–º
2. –ü—Ä–æ–≤–µ—Ä—å –í–°–ï –∞—Ç—Ä–∏–±—É—Ç—ã —Ç–æ–≤–∞—Ä–∞ - –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –ö–¢–†–£
3. –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—å –º–∞–ª–µ–π—à–∏–µ —Å–æ–º–Ω–µ–Ω–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ - –æ—Ç–≤–µ—á–∞–π "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω"
4. –ù–ï –≤—ã–±–∏—Ä–∞–π –∫–æ–¥ –æ–±—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∫–æ–¥
5. –£—á–∏—Ç—ã–≤–∞–π –í–°–ï –¥–µ—Ç–∞–ª–∏: –±—Ä–µ–Ω–¥, –º–æ–¥–µ–ª—å, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

–¢–û–í–ê–† –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:
"""

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–µ
        prompt += f"–ù–∞–∑–≤–∞–Ω–∏–µ: {sku_data.get('title', '')}\n"
        if sku_data.get('description'):
            prompt += f"–û–ø–∏—Å–∞–Ω–∏–µ: {sku_data.get('description', '')}\n"
        if sku_data.get('category'):
            prompt += f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {sku_data.get('category', '')}\n"
        if sku_data.get('brand'):
            prompt += f"–ë—Ä–µ–Ω–¥: {sku_data.get('brand', '')}\n"

        # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã
        if sku_attrs:
            prompt += "\n–ê–¢–†–ò–ë–£–¢–´ –¢–û–í–ê–†–ê:\n"
            for attr_name, attr_value in sku_attrs.items():
                prompt += f"- {attr_name}: {attr_value}\n"

        prompt += "\n–ö–ê–ù–î–ò–î–ê–¢–´ –ò–ó –ö–¢–†–£ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏):\n"

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        for i, entry in enumerate(similar_ktru_entries[:10], 1):  # –¢–æ–ø-10 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            payload = entry.payload
            score = getattr(entry, 'score', 0)
            ktru_attrs = self._extract_attributes(payload)
            attr_similarity = self._calculate_attribute_similarity(sku_attrs, ktru_attrs)

            prompt += f"\n{i}. –ö–û–î: {payload.get('ktru_code', '')}\n"
            prompt += f"   –ù–ê–ó–í–ê–ù–ò–ï: {payload.get('title', '')}\n"
            prompt += f"   –°–•–û–ñ–ï–°–¢–¨ –¢–ï–ö–°–¢–ê: {score:.3f}\n"
            prompt += f"   –°–•–û–ñ–ï–°–¢–¨ –ê–¢–†–ò–ë–£–¢–û–í: {attr_similarity:.2f}\n"

            if ktru_attrs:
                prompt += "   –ê–¢–†–ò–ë–£–¢–´ –ö–¢–†–£:\n"
                for attr_name, attr_value in list(ktru_attrs.items())[:5]:  # –ü–µ—Ä–≤—ã–µ 5 –∞—Ç—Ä–∏–±—É—Ç–æ–≤
                    prompt += f"   - {attr_name}: {attr_value}\n"

        prompt += """
–ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –í–´–ë–û–†–£:
1. –ù–∞–π–¥–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞, –≥–¥–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ò –∞—Ç—Ä–∏–±—É—Ç—ã –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç–æ–≤–∞—Ä—É
2. –ï—Å–ª–∏ —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ > 0.9 –ò —Å—Ö–æ–∂–µ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç–æ–≤ > 0.7 - —ç—Ç–æ —Ö–æ—Ä–æ—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç
3. –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞ –µ—Å—Ç—å –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –ö–¢–†–£
4. –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ–¥—Ö–æ–¥—è—Ç - –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π (–Ω–µ –æ–±—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é)
5. –ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω –∫–∞–Ω–¥–∏–¥–∞—Ç –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é 95% - –æ—Ç–≤–µ—Ç—å "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω"

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
- –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω —Ç–æ—á–Ω—ã–π –∫–æ–¥: –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –∫–æ–¥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ XX.XX.XX.XXX-XXXXXXXX
- –ï—Å–ª–∏ –∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω: –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û —Ñ—Ä–∞–∑—É "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω"

–û–¢–í–ï–¢:"""

        return prompt

    def _validate_ktru_match(self, sku_data: Dict, ktru_data: Dict,
                             text_similarity: float, attr_similarity: float) -> Tuple[bool, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è SKU –∏ KTRU"""
        # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence = 0.0

        # –í–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ (70%)
        if text_similarity >= SIMILARITY_THRESHOLD_HIGH:
            confidence += 0.7
        elif text_similarity >= SIMILARITY_THRESHOLD_MEDIUM:
            confidence += 0.5
        elif text_similarity >= SIMILARITY_THRESHOLD_LOW:
            confidence += 0.3
        else:
            return False, 0.0

        # –í–∫–ª–∞–¥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ (30%)
        if ENABLE_ATTRIBUTE_MATCHING:
            confidence += attr_similarity * ATTRIBUTE_WEIGHT

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        sku_title_lower = sku_data.get('title', '').lower()
        ktru_title_lower = ktru_data.get('title', '').lower()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        sku_keywords = set(sku_title_lower.split())
        ktru_keywords = set(ktru_title_lower.split())

        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ö–æ—Ç—è –±—ã 30% –æ–±—â–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if len(sku_keywords) > 0 and len(ktru_keywords) > 0:
            common_keywords = sku_keywords.intersection(ktru_keywords)
            keyword_overlap = len(common_keywords) / min(len(sku_keywords), len(ktru_keywords))
            if keyword_overlap < 0.3:
                confidence *= 0.7  # –°–Ω–∏–∂–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'category' in sku_data and sku_data['category']:
            sku_category = sku_data['category'].lower()
            if sku_category not in ktru_title_lower and sku_category not in ktru_data.get('description', '').lower():
                confidence *= 0.8  # –°–Ω–∏–∂–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        is_valid = confidence >= CLASSIFICATION_CONFIDENCE_THRESHOLD

        return is_valid, confidence

    def classify_sku(self, sku_data: Dict, top_k: int = TOP_K) -> Dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è SKU –ø–æ –ö–¢–†–£ –∫–æ–¥—É —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {sku_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")

        if not self.qdrant_client:
            logger.error("‚ùå Qdrant –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ SKU
            sku_attrs = self._extract_attributes(sku_data)
            logger.info(f"üìã –ò–∑–≤–ª–µ—á–µ–Ω–æ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ SKU: {len(sku_attrs)}")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            sku_text_parts = [
                sku_data.get('title', ''),
                sku_data.get('description', ''),
                sku_data.get('category', ''),
                sku_data.get('brand', '')
            ]

            # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã
            for attr_name, attr_value in sku_attrs.items():
                sku_text_parts.append(f"{attr_name}: {attr_value}")

            sku_text = ' '.join(filter(None, sku_text_parts))
            logger.info(f"üìù –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞: {sku_text[:100]}...")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            sku_embedding = generate_embedding(sku_text)
            logger.info(f"üî¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(sku_embedding)}")

            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ö–¢–†–£ –∫–æ–¥–æ–≤
            search_result = self.qdrant_client.search(
                collection_name=QDRANT_COLLECTION,
                query_vector=sku_embedding.tolist(),
                limit=top_k
            )

            if not search_result:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –ö–¢–†–£ –∑–∞–ø–∏—Å–µ–π")
                return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            best_candidates = []

            for result in search_result:
                score = getattr(result, 'score', 0)
                payload = result.payload

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã KTRU
                ktru_attrs = self._extract_attributes(payload)

                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç–æ–≤
                attr_similarity = self._calculate_attribute_similarity(sku_attrs, ktru_attrs)

                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
                is_valid, confidence = self._validate_ktru_match(
                    sku_data, payload, score, attr_similarity
                )

                if is_valid:
                    best_candidates.append({
                        'result': result,
                        'confidence': confidence,
                        'text_similarity': score,
                        'attr_similarity': attr_similarity
                    })

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            best_candidates.sort(key=lambda x: x['confidence'], reverse=True)

            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç —Å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
            if best_candidates and best_candidates[0]['confidence'] >= 0.98:
                best = best_candidates[0]
                payload = best['result'].payload
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {best['confidence']:.3f}")
                return {
                    "ktru_code": payload.get('ktru_code', '–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω'),
                    "ktru_title": payload.get('title', None),
                    "confidence": best['confidence']
                }

            # –ï—Å–ª–∏ –Ω–µ—Ç LLM –º–æ–¥–µ–ª–∏ –∏ –Ω–µ—Ç –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if not self.llm or not self.tokenizer:
                if best_candidates:
                    best = best_candidates[0]
                    if best['confidence'] >= CLASSIFICATION_CONFIDENCE_THRESHOLD:
                        payload = best['result'].payload
                        return {
                            "ktru_code": payload.get('ktru_code', '–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω'),
                            "ktru_title": payload.get('title', None),
                            "confidence": best['confidence']
                        }

                logger.warning("‚ö†Ô∏è LLM –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏ –Ω–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é")
                return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
            prompt = self._create_advanced_prompt(sku_data, search_result[:20], sku_attrs)
            logger.info(f"üìã –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)
            inputs = {k: v.to(self.llm.device) for k, v in inputs.items()}

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            generation_config = GenerationConfig(
                temperature=TEMPERATURE,
                top_p=TOP_P,
                repetition_penalty=REPETITION_PENALTY,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            logger.info("ü§ñ –ó–∞–ø—É—Å–∫ LLM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
            with torch.no_grad():
                generated_ids = self.llm.generate(
                    **inputs,
                    generation_config=generation_config
                )

            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            response = response[len(prompt):].strip()

            logger.info(f"ü§ñ –û—Ç–≤–µ—Ç LLM: '{response}'")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ö–¢–†–£ –∫–æ–¥–∞ –≤ –æ—Ç–≤–µ—Ç–µ
            ktru_match = self.ktru_pattern.search(response)

            if ktru_match:
                ktru_code = ktru_match.group(0)

                # –ù–∞—Ö–æ–¥–∏–º —ç—Ç–æ—Ç –∫–æ–¥ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                for candidate in best_candidates:
                    if candidate['result'].payload.get('ktru_code') == ktru_code:
                        logger.info(f"‚úÖ LLM –≤—ã–±—Ä–∞–ª –∫–æ–¥: {ktru_code} —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {candidate['confidence']:.3f}")
                        return {
                            "ktru_code": ktru_code,
                            "ktru_title": candidate['result'].payload.get('title', None),
                            "confidence": candidate['confidence']
                        }

                # –ï—Å–ª–∏ –∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞—Ö, –∏—â–µ–º –≤ –±–∞–∑–µ
                ktru_title = self._find_ktru_title_by_code(ktru_code, search_result)
                return {
                    "ktru_code": ktru_code,
                    "ktru_title": ktru_title,
                    "confidence": 0.95  # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è LLM
                }

            elif "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω" in response.lower():
                logger.info("‚ùå LLM –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ")
                return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç LLM: '{response}'")
                return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ SKU: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

    def _find_ktru_title_by_code(self, ktru_code: str, search_results: List) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è –ö–¢–†–£ –ø–æ –∫–æ–¥—É"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
            for entry in search_results:
                payload = entry.payload
                if payload.get('ktru_code') == ktru_code:
                    return payload.get('title', '')

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º –≤ –±–∞–∑–µ
            if self.qdrant_client:
                try:
                    scroll_result = self.qdrant_client.scroll(
                        collection_name=QDRANT_COLLECTION,
                        scroll_filter=Filter(
                            must=[
                                FieldCondition(
                                    key="ktru_code",
                                    match=MatchValue(value=ktru_code)
                                )
                            ]
                        ),
                        limit=1,
                        with_payload=True,
                        with_vectors=False
                    )

                    points, _ = scroll_result
                    if points:
                        return points[0].payload.get('title', '')
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –±–∞–∑–µ: {e}")

            return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ö–¢–†–£: {e}")
            return None


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
classifier = KtruClassifier()


def classify_sku(sku_data: Dict, top_k: int = TOP_K) -> Dict:
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ SKU"""
    return classifier.classify_sku(sku_data, top_k)