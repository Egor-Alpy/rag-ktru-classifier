import re
import torch
import json
import logging
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer, GenerationConfig, LlamaTokenizer
from qdrant_client import QdrantClient
from embedding import generate_embedding
from config import (
    LLM_BASE_MODEL, LLM_ADAPTER_MODEL, QDRANT_HOST, QDRANT_PORT,
    QDRANT_COLLECTION, TEMPERATURE, TOP_P, REPETITION_PENALTY,
    MAX_NEW_TOKENS, TOP_K
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class KtruClassifier:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ö–¢–†–£"""
        self.qdrant_client = self._setup_qdrant()
        self.llm, self.tokenizer = self._setup_llm()

        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ö–¢–†–£ –∫–æ–¥–∞
        self.ktru_pattern = re.compile(r'\d{2}\.\d{2}\.\d{2}\.\d{3}-\d{8}')

    def _setup_qdrant(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant"""
        try:
            qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            collections = qdrant_client.get_collections()
            logger.info(f"‚úÖ Qdrant –ø–æ–¥–∫–ª—é—á–µ–Ω, –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections.collections)}")
            return qdrant_client
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant: {e}")
            return None

    def _setup_llm(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {LLM_BASE_MODEL}")

            # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞
            tokenizer = None
            model = None

            # –°–ø–æ—Å–æ–± 1: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
            try:
                logger.info("–ü–æ–ø—ã—Ç–∫–∞ 1: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞")
                tokenizer = AutoTokenizer.from_pretrained(
                    LLM_BASE_MODEL,
                    trust_remote_code=True,
                    use_fast=False  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä
                )
                logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ø–æ—Å–æ–±)")
            except Exception as e:
                logger.warning(f"–°–ø–æ—Å–æ–± 1 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")

            # –°–ø–æ—Å–æ–± 2: –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞–∫ LlamaTokenizer
            if tokenizer is None:
                try:
                    logger.info("–ü–æ–ø—ã—Ç–∫–∞ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞–∫ LlamaTokenizer")
                    tokenizer = LlamaTokenizer.from_pretrained(
                        LLM_BASE_MODEL,
                        trust_remote_code=True
                    )
                    logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (LlamaTokenizer)")
                except Exception as e:
                    logger.warning(f"–°–ø–æ—Å–æ–± 2 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")

            # –°–ø–æ—Å–æ–± 3: –ó–∞–≥—Ä—É–∑–∫–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            if tokenizer is None:
                try:
                    logger.info("–ü–æ–ø—ã—Ç–∫–∞ 3: –ó–∞–≥—Ä—É–∑–∫–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
                    tokenizer = AutoTokenizer.from_pretrained(
                        LLM_BASE_MODEL,
                        trust_remote_code=True,
                        use_fast=False,
                        legacy=True,
                        padding_side="left"
                    )
                    logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)")
                except Exception as e:
                    logger.warning(f"–°–ø–æ—Å–æ–± 3 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")

            # –°–ø–æ—Å–æ–± 4: –ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å
            if tokenizer is None:
                logger.info("–ü–æ–ø—ã—Ç–∫–∞ 4: –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å")
                alternative_model = "microsoft/DialoGPT-medium"
                try:
                    tokenizer = AutoTokenizer.from_pretrained(
                        alternative_model,
                        trust_remote_code=True
                    )
                    LLM_BASE_MODEL_ACTUAL = alternative_model
                    logger.info(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å: {alternative_model})")
                except Exception as e:
                    logger.error(f"–í—Å–µ —Å–ø–æ—Å–æ–±—ã –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏: {e}")
                    return None, None
            else:
                LLM_BASE_MODEL_ACTUAL = LLM_BASE_MODEL

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—É —Å —Ç–æ–∫–µ–Ω–∞–º–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if tokenizer.pad_token_id is None:
                tokenizer.pad_token_id = tokenizer.eos_token_id

            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ç–æ–∫–µ–Ω—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
            if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:
                if tokenizer.pad_token_id != tokenizer.eos_token_id:
                    tokenizer.pad_token_id = tokenizer.eos_token_id

            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–µ—Ä–∞ –º–æ–¥–µ–ª–∏: {LLM_ADAPTER_MODEL}")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏
            device_available = torch.cuda.is_available()
            logger.info(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {device_available}")

            if device_available:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É bfloat16 —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞
                try:
                    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ bfloat16
                    test_tensor = torch.tensor([1.0], dtype=torch.bfloat16, device='cuda')
                    torch_dtype = torch.bfloat16
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è bfloat16")
                except (RuntimeError, AssertionError, Exception):
                    torch_dtype = torch.float16
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è float16 (bfloat16 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)")
            else:
                torch_dtype = torch.float32
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è float32 (CPU —Ä–µ–∂–∏–º)")

            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
            try:
                model = AutoPeftModelForCausalLM.from_pretrained(
                    LLM_ADAPTER_MODEL,
                    device_map="auto",
                    torch_dtype=torch_dtype
                )
                logger.info("‚úÖ PEFT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PEFT –º–æ–¥–µ–ª–∏: {e}")
                logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞...")

                try:
                    from transformers import AutoModelForCausalLM
                    model = AutoModelForCausalLM.from_pretrained(
                        LLM_BASE_MODEL_ACTUAL,
                        device_map="auto",
                        torch_dtype=torch_dtype
                    )
                    logger.info("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                except Exception as e2:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e2}")
                    return None, None

            return model, tokenizer

        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return None, None

    def create_simple_prompt(self, sku_data, similar_ktru_entries):
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤ –ö–¢–†–£. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–π –∫–æ–¥ –ö–¢–†–£ –¥–ª—è —Ç–æ–≤–∞—Ä–∞.

–¢–û–í–ê–† –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:
–ù–∞–∑–≤–∞–Ω–∏–µ: """ + sku_data.get('title', '') + """
–û–ø–∏—Å–∞–Ω–∏–µ: """ + sku_data.get('description', '') + """

–ü–û–•–û–ñ–ò–ï –¢–û–í–ê–†–´ –ò–ó –ö–¢–†–£:
"""

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏ –ö–¢–†–£ –≤ –ø—Ä–æ–º–ø—Ç (—Ç–æ–ª—å–∫–æ —Ç–æ–ø-3 –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏)
        for i, entry in enumerate(similar_ktru_entries[:3], 1):
            payload = entry.payload
            score = getattr(entry, 'score', 0)
            prompt += f"\n{i}. –ö–û–î: {payload.get('ktru_code', '')} | –ù–ê–ó–í–ê–ù–ò–ï: {payload.get('title', '')} | –°–•–û–ñ–ï–°–¢–¨: {score:.3f}\n"

        prompt += """
–ò–ù–°–¢–†–£–ö–¶–ò–Ø: 
–í—ã–±–µ—Ä–∏ –¢–û–ß–ù–û –û–î–ò–ù –∫–æ–¥ –ö–¢–†–£, –∫–æ—Ç–æ—Ä—ã–π –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç —Ç–æ–≤–∞—Ä—É.
–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –∫–æ–¥–æ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ XX.XX.XX.XXX-XXXXXXXX
–ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –∫–æ–¥–∞, –æ—Ç–≤–µ—Ç—å: –∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω

–û–¢–í–ï–¢:"""

        return prompt

    def _find_ktru_title_by_code(self, ktru_code, search_results):
        """–ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è –ö–¢–†–£ –ø–æ –∫–æ–¥—É –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
            for entry in search_results:
                payload = entry.payload
                if payload.get('ktru_code') == ktru_code:
                    title = payload.get('title', '')
                    logger.debug(f"–ù–∞–π–¥–µ–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞: {title}")
                    return title

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞, –∏—â–µ–º –≤ –±–∞–∑–µ
            if self.qdrant_client:
                try:
                    scroll_result = self.qdrant_client.scroll(
                        collection_name=QDRANT_COLLECTION,
                        scroll_filter={
                            "must": [
                                {
                                    "key": "ktru_code",
                                    "match": {"value": ktru_code}
                                }
                            ]
                        },
                        limit=1,
                        with_payload=True,
                        with_vectors=False
                    )

                    points, next_page_offset = scroll_result
                    if points:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        title = points[0].payload.get('title', '')
                        logger.debug(f"–ù–∞–π–¥–µ–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –±–∞–∑–µ: {title}")
                        return title
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –±–∞–∑–µ: {e}")

            logger.warning(f"–ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –∫–æ–¥–∞ {ktru_code} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ö–¢–†–£: {e}")
            return None

    def _debug_search_results(self, search_results, query_text):
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞"""
        logger.info(f"üîç –û—Ç–ª–∞–¥–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è: '{query_text[:50]}...'")
        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(search_results)}")

        for i, entry in enumerate(search_results[:5], 1):
            payload = entry.payload
            score = getattr(entry, 'score', 0)
            logger.info(f"  {i}. –ö–æ–¥: {payload.get('ktru_code', 'N/A')}")
            logger.info(f"     –ù–∞–∑–≤–∞–Ω–∏–µ: {payload.get('title', 'N/A')[:60]}...")
            logger.info(f"     –°—Ö–æ–∂–µ—Å—Ç—å: {score:.3f}")
        logger.info("=" * 50)

    def classify_sku(self, sku_data, top_k=TOP_K):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è SKU –ø–æ –ö–¢–†–£ –∫–æ–¥—É —Å –æ—Ç–ª–∞–¥–∫–æ–π"""
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {sku_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")

        if not self.qdrant_client:
            logger.error("‚ùå Qdrant –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            sku_text = f"{sku_data['title']} {sku_data.get('description', '')}"

            # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã
            if 'attributes' in sku_data and sku_data['attributes']:
                for attr in sku_data['attributes']:
                    attr_name = attr.get('attr_name', '')
                    attr_value = attr.get('attr_value', '')
                    sku_text += f" {attr_name}: {attr_value}"

            logger.info(f"üìù –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞: {sku_text}")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            sku_embedding = generate_embedding(sku_text)
            logger.info(f"üî¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(sku_embedding)}")

            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ö–¢–†–£ –∫–æ–¥–æ–≤
            search_result = self.qdrant_client.search(
                collection_name=QDRANT_COLLECTION,
                query_vector=sku_embedding.tolist(),
                limit=top_k
            )

            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            self._debug_search_results(search_result, sku_text)

            if not search_result:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –ö–¢–†–£ –∑–∞–ø–∏—Å–µ–π")
                return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
            best_result = search_result[0]
            best_score = getattr(best_result, 'score', 0)
            best_payload = best_result.payload

            logger.info(f"üèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
            logger.info(f"   –ö–æ–¥: {best_payload.get('ktru_code', 'N/A')}")
            logger.info(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {best_payload.get('title', 'N/A')}")
            logger.info(f"   –°—Ö–æ–∂–µ—Å—Ç—å: {best_score:.3f}")

            # –ï—Å–ª–∏ —Å—Ö–æ–∂–µ—Å—Ç—å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è (>0.85), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑ LLM
            if best_score > 0.85:
                logger.info(f"‚úÖ –í—ã—Å–æ–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å ({best_score:.3f}), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                return {
                    "ktru_code": best_payload.get('ktru_code', '–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω'),
                    "ktru_title": best_payload.get('title', None),
                    "confidence": best_score
                }

            # –ï—Å–ª–∏ –Ω–µ—Ç LLM –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            if not self.llm or not self.tokenizer:
                logger.warning("‚ö†Ô∏è LLM –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return self._classify_by_similarity_only(sku_data, top_k)

            # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            prompt = self.create_simple_prompt(sku_data, search_result)
            logger.info(f"üìã –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            inputs = {k: v.to(self.llm.device) for k, v in inputs.items()}

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
            generation_config = GenerationConfig(
                temperature=0.1,  # –ú–µ–Ω—å—à–µ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
                top_p=0.9,
                repetition_penalty=1.1,
                max_new_tokens=50,  # –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            logger.info("ü§ñ –ó–∞–ø—É—Å–∫ LLM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
            with torch.no_grad():
                generated_ids = self.llm.generate(
                    **inputs,
                    generation_config=generation_config
                )

            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            response = response[len(prompt):].strip()

            logger.info(f"ü§ñ –û—Ç–≤–µ—Ç LLM: '{response}'")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ö–¢–†–£ –∫–æ–¥–∞ –≤ –æ—Ç–≤–µ—Ç–µ
            ktru_match = self.ktru_pattern.search(response)

            if ktru_match:
                ktru_code = ktru_match.group(0)
                ktru_title = self._find_ktru_title_by_code(ktru_code, search_result)
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–¥: {ktru_code}, –Ω–∞–∑–≤–∞–Ω–∏–µ: {ktru_title}")
                return {
                    "ktru_code": ktru_code,
                    "ktru_title": ktru_title,
                    "confidence": 0.9  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                }
            elif "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω" in response.lower():
                logger.info("‚ùå LLM –æ—Ç–≤–µ—Ç–∏–ª: –∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}
            else:
                # –ï—Å–ª–∏ LLM –Ω–µ –¥–∞–ª —á–µ—Ç–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞
                logger.warning(f"‚ö†Ô∏è –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç LLM: '{response}', –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞")
                if best_score > 0.7:  # –ü–æ–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥
                    return {
                        "ktru_code": best_payload.get('ktru_code', '–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω'),
                        "ktru_title": best_payload.get('title', None),
                        "confidence": best_score
                    }
                else:
                    return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ SKU: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

    def _classify_by_similarity_only(self, sku_data, top_k=TOP_K):
        """Fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            sku_text = f"{sku_data['title']} {sku_data.get('description', '')}"

            # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã
            if 'attributes' in sku_data and sku_data['attributes']:
                for attr in sku_data['attributes']:
                    attr_name = attr.get('attr_name', '')
                    attr_value = attr.get('attr_value', '')
                    sku_text += f" {attr_name}: {attr_value}"

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            sku_embedding = generate_embedding(sku_text)

            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ö–¢–†–£ –∫–æ–¥–æ–≤
            search_result = self.qdrant_client.search(
                collection_name=QDRANT_COLLECTION,
                query_vector=sku_embedding.tolist(),
                limit=1  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–π –ø–æ—Ö–æ–∂–∏–π
            )

            if search_result and len(search_result) > 0:
                best_match = search_result[0]
                confidence = getattr(best_match, 'score', 0)

                # –ü–æ–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                if confidence > 0.65:  # –ë—ã–ª–æ 0.75
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏: {confidence:.3f}")
                    ktru_code = best_match.payload.get('ktru_code', '–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω')
                    ktru_title = best_match.payload.get('title', None)
                    return {
                        "ktru_code": ktru_code,
                        "ktru_title": ktru_title,
                        "confidence": confidence
                    }
                else:
                    logger.info(f"‚ùå –°—Ö–æ–∂–µ—Å—Ç—å —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è: {confidence:.3f}")
                    return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}
            else:
                return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
classifier = KtruClassifier()


def classify_sku(sku_data, top_k=TOP_K):
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ SKU"""
    return classifier.classify_sku(sku_data, top_k)