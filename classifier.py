import re
import torch
import json
import logging
from typing import List, Dict, Optional, Tuple, Set
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer, GenerationConfig, LlamaTokenizer
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
from embedding import generate_embedding
from config import (
    LLM_BASE_MODEL, LLM_ADAPTER_MODEL, QDRANT_HOST, QDRANT_PORT,
    QDRANT_COLLECTION, TEMPERATURE, TOP_P, REPETITION_PENALTY,
    MAX_NEW_TOKENS, TOP_K, SIMILARITY_THRESHOLD_HIGH,
    SIMILARITY_THRESHOLD_MEDIUM, SIMILARITY_THRESHOLD_LOW,
    CLASSIFICATION_CONFIDENCE_THRESHOLD, ENABLE_ATTRIBUTE_MATCHING,
    ATTRIBUTE_WEIGHT
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class KtruClassifier:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ö–¢–†–£"""
        self.qdrant_client = self._setup_qdrant()
        self.llm, self.tokenizer = self._setup_llm()

        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ö–¢–†–£ –∫–æ–¥–∞
        self.ktru_pattern = re.compile(r'\d{2}\.\d{2}\.\d{2}\.\d{3}-\d{8}')

        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        self.attribute_normalization = {
            # –û–±—â–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤": ["—Å–ª–æ–π–Ω–æ—Å—Ç—å", "—á–∏—Å–ª–æ —Å–ª–æ–µ–≤", "—Å–ª–æ–∏", "—Å–ª–æ–π–Ω—ã–π"],
            "—Ü–≤–µ—Ç": ["–æ–∫—Ä–∞—Å–∫–∞", "—Ä–∞—Å—Ü–≤–µ—Ç–∫–∞", "–æ—Ç—Ç–µ–Ω–æ–∫", "—Ü–≤–µ—Ç –±—É–º–∞–≥–∏"],
            "—Ç–∏–ø": ["–≤–∏–¥", "—Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç—å", "–∫–∞—Ç–µ–≥–æ—Ä–∏—è"],
            "–º–∞—Ç–µ—Ä–∏–∞–ª": ["—Å–æ—Å—Ç–∞–≤", "—Å—ã—Ä—å–µ", "–æ—Å–Ω–æ–≤–∞"],
            "—Ä–∞–∑–º–µ—Ä": ["–≥–∞–±–∞—Ä–∏—Ç", "–≤–µ–ª–∏—á–∏–Ω–∞", "—Ñ–æ—Ä–º–∞—Ç", "—Ä–∞–∑–º–µ—Ä—ã"],
            "–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ": ["–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ", "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ", "—Ü–µ–ª—å", "–¥–ª—è"],

            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            "–æ–±—ä–µ–º": ["–æ–±—ä—ë–º", "–µ–º–∫–æ—Å—Ç—å", "–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å", "–æ–±—ä—ë–º —Ä–µ–∞–≥–µ–Ω—Ç–∞"],
            "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä": ["cpu", "—á–∏–ø", "–º–∏–∫—Ä–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"],
            "–ø–∞–º—è—Ç—å": ["ram", "–æ–∑—É", "–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å"],
            "—Ç–æ—Ä–≥–æ–≤–∞—è –º–∞—Ä–∫–∞": ["–±—Ä–µ–Ω–¥", "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å", "–º–∞—Ä–∫–∞", "brand"],

            # –ï–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
            "—à—Ç—É–∫–∞": ["—à—Ç", "–µ–¥–∏–Ω–∏—Ü–∞", "–µ–¥"],
            "–Ω–∞–±–æ—Ä": ["–∫–æ–º–ø–ª–µ–∫—Ç", "set", "kit"],
            "—É–ø–∞–∫–æ–≤–∫–∞": ["–ø–∞—á–∫–∞", "–ø–∞–∫–µ—Ç", "—É–ø"],
        }

        # –í–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–≤–∞—Ä–æ–≤
        self.category_keywords = {
            "–∫–æ–º–ø—å—é—Ç–µ—Ä": ["–Ω–æ—É—Ç–±—É–∫", "laptop", "notebook", "–ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä", "–ø–∫", "pc"],
            "–±—É–º–∞–≥–∞": ["—Ç—É–∞–ª–µ—Ç–Ω–∞—è", "–æ—Ñ–∏—Å–Ω–∞—è", "–ø–∏—Å—á–∞—è", "–∫–æ–ø–∏—Ä–æ–≤–∞–ª—å–Ω–∞—è"],
            "—Ä–µ–∞–≥–µ–Ω—Ç": ["–∏–≤–¥", "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", "–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–π", "–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä"],
            "–∫–∞–Ω—Ü–µ–ª—è—Ä–∏—è": ["—Ä—É—á–∫–∞", "–∫–∞—Ä–∞–Ω–¥–∞—à", "–º–∞—Ä–∫–µ—Ä", "—Ñ–ª–æ–º–∞—Å—Ç–µ—Ä"],
            "–º–µ–±–µ–ª—å": ["—Å—Ç–æ–ª", "—Å—Ç—É–ª", "—à–∫–∞—Ñ", "—Ç—É–º–±–∞", "–∫—Ä–µ—Å–ª–æ"],
        }

    def _setup_qdrant(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant"""
        try:
            qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
            collections = qdrant_client.get_collections()
            logger.info(f"‚úÖ Qdrant –ø–æ–¥–∫–ª—é—á–µ–Ω, –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections.collections)}")
            return qdrant_client
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant: {e}")
            return None

    def _setup_llm(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {LLM_BASE_MODEL}")

            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞
            tokenizer = None
            model = None

            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    LLM_BASE_MODEL,
                    trust_remote_code=True,
                    use_fast=False
                )
                logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞: {e}")
                try:
                    tokenizer = LlamaTokenizer.from_pretrained(
                        LLM_BASE_MODEL,
                        trust_remote_code=True
                    )
                    logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (LlamaTokenizer)")
                except Exception as e2:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä: {e2}")
                    return None, None

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–µ—Ä–∞
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            if tokenizer.pad_token_id is None:
                tokenizer.pad_token_id = tokenizer.eos_token_id

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
            device_available = torch.cuda.is_available()
            logger.info(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {device_available}")

            if device_available:
                try:
                    test_tensor = torch.tensor([1.0], dtype=torch.bfloat16, device='cuda')
                    torch_dtype = torch.bfloat16
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è bfloat16")
                except:
                    torch_dtype = torch.float16
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è float16")
            else:
                torch_dtype = torch.float32
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è float32 (CPU —Ä–µ–∂–∏–º)")

            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            try:
                model = AutoPeftModelForCausalLM.from_pretrained(
                    LLM_ADAPTER_MODEL,
                    device_map="auto",
                    torch_dtype=torch_dtype
                )
                logger.info("‚úÖ PEFT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PEFT –º–æ–¥–µ–ª–∏: {e}")
                try:
                    from transformers import AutoModelForCausalLM
                    model = AutoModelForCausalLM.from_pretrained(
                        LLM_BASE_MODEL,
                        device_map="auto",
                        torch_dtype=torch_dtype
                    )
                    logger.info("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                except Exception as e2:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e2}")
                    return None, None

            return model, tokenizer

        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return None, None

    def _normalize_attribute_name(self, attr_name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–∞"""
        attr_lower = attr_name.lower().strip()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–æ–≤–∞—Ä—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        for normalized, variants in self.attribute_normalization.items():
            if attr_lower == normalized or attr_lower in variants:
                return normalized

        return attr_lower

    def _extract_attributes(self, data: Dict) -> Dict[str, Set[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        attributes = {}

        if 'attributes' in data and isinstance(data['attributes'], list):
            for attr in data['attributes']:
                if isinstance(attr, dict):
                    # –î–ª—è SKU —Ñ–æ—Ä–º–∞—Ç
                    if 'attr_name' in attr and 'attr_value' in attr:
                        name = self._normalize_attribute_name(attr['attr_name'])
                        value = str(attr['attr_value']).lower().strip()
                        if name not in attributes:
                            attributes[name] = set()
                        attributes[name].add(value)

                    # –î–ª—è KTRU —Ñ–æ—Ä–º–∞—Ç
                    elif 'attr_name' in attr and 'attr_values' in attr:
                        name = self._normalize_attribute_name(attr['attr_name'])
                        if name not in attributes:
                            attributes[name] = set()

                        for val in attr['attr_values']:
                            if isinstance(val, dict) and 'value' in val:
                                value = str(val['value']).lower().strip()
                                # –î–æ–±–∞–≤–ª—è–µ–º –µ–¥–∏–Ω–∏—Ü—É –∏–∑–º–µ—Ä–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
                                if 'value_unit' in val and val['value_unit']:
                                    unit = val['value_unit'].strip()
                                    if unit:
                                        value = f"{value} {unit}"
                                attributes[name].add(value)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        result = {}
        for name, values in attributes.items():
            result[name] = '; '.join(sorted(values))

        return result

    def _calculate_attribute_similarity(self, sku_attrs: Dict[str, str], ktru_attrs: Dict[str, str]) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤"""
        if not sku_attrs or not ktru_attrs:
            return 0.0

        matches = 0
        total_comparisons = 0
        critical_matches = 0  # –î–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        critical_attributes = {
            "—Ç–∏–ø", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤", "–º–∞—Ç–µ—Ä–∏–∞–ª", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "–ø–∞–º—è—Ç—å",
            "–æ–±—ä–µ–º", "–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ", "—Ä–∞–∑–º–µ—Ä"
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        for sku_attr_name, sku_attr_value in sku_attrs.items():
            if sku_attr_name in ktru_attrs:
                ktru_value = ktru_attrs[sku_attr_name]
                total_comparisons += 1
                is_critical = sku_attr_name in critical_attributes

                # –†–∞–∑–±–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                sku_values = set(v.strip() for v in sku_attr_value.split(';'))
                ktru_values = set(v.strip() for v in ktru_value.split(';'))

                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                if sku_values.intersection(ktru_values):
                    matches += 1
                    if is_critical:
                        critical_matches += 1
                # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                else:
                    partial_match = False
                    for sv in sku_values:
                        for kv in ktru_values:
                            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
                            if sv in kv or kv in sv:
                                partial_match = True
                                break
                            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
                            if self._check_numeric_match(sv, kv):
                                partial_match = True
                                break
                        if partial_match:
                            break

                    if partial_match:
                        matches += 0.7 if is_critical else 0.5

        if total_comparisons == 0:
            return 0.0

        # –ë–∞–∑–æ–≤–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
        base_similarity = matches / total_comparisons

        # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        critical_bonus = critical_matches * 0.1

        return min(1.0, base_similarity + critical_bonus)

    def _check_numeric_match(self, value1: str, value2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞ –∏–∑ —Å—Ç—Ä–æ–∫
            nums1 = re.findall(r'[\d.]+', value1)
            nums2 = re.findall(r'[\d.]+', value2)

            if not nums1 or not nums2:
                return False

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã (‚â•, ‚â§, –∏ —Ç.–¥.)
            if '‚â•' in value2 or '>=' in value2:
                return float(nums1[0]) >= float(nums2[0])
            elif '‚â§' in value2 or '<=' in value2:
                return float(nums1[0]) <= float(nums2[0])
            else:
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –¥–æ–ø—É—Å–∫–æ–º 10%
                num1 = float(nums1[0])
                num2 = float(nums2[0])
                return abs(num1 - num2) / max(num1, num2) < 0.1
        except:
            return False

    def _create_search_text(self, sku_data: Dict, sku_attrs: Dict[str, str]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        text_parts = []

        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if sku_data.get('title'):
            text_parts.append(sku_data['title'])

        if sku_data.get('category'):
            text_parts.append(f"–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {sku_data['category']}")

        if sku_data.get('brand'):
            text_parts.append(f"–±—Ä–µ–Ω–¥: {sku_data['brand']}")

        if sku_data.get('description'):
            text_parts.append(sku_data['description'])

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∞—Ç—Ä–∏–±—É—Ç—ã –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ
        for attr_name, attr_value in sku_attrs.items():
            text_parts.append(f"{attr_name}: {attr_value}")

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        text_lower = ' '.join(text_parts).lower()
        for category, keywords in self.category_keywords.items():
            if any(kw in text_lower for kw in keywords):
                text_parts.append(f"–∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞: {category}")

        return ' '.join(text_parts)

    def _create_classification_prompt(self, sku_data: Dict, similar_ktru_entries: List,
                                      sku_attrs: Dict[str, str]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ –ö–¢–†–£. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –Ω–∞–π—Ç–∏ –ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –¢–û–ß–ù–´–ô –∫–æ–¥ –ö–¢–†–£.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ê–í–ò–õ–ê:
1. –ö–æ–¥ –¥–æ–ª–∂–µ–Ω –¢–û–ß–ù–û —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ç–æ–≤–∞—Ä—É –ø–æ –í–°–ï–ú —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º
2. –ü—Ä–æ–≤–µ—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ, –º–∞—Ç–µ—Ä–∏–∞–ª—ã
3. –ù–ï –≤—ã–±–∏—Ä–∞–π –ø–æ—Ö–æ–∂–∏–π –∫–æ–¥ - —Ç–æ–ª—å–∫–æ –¢–û–ß–ù–û–ï —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
4. –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ–º–Ω–µ–Ω–∏—è - –æ—Ç–≤–µ—Ç—å "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω"

–¢–û–í–ê–† –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:
"""
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–≤–∞—Ä–µ
        prompt += f"–ù–∞–∑–≤–∞–Ω–∏–µ: {sku_data.get('title', '')}\n"
        if sku_data.get('description'):
            prompt += f"–û–ø–∏—Å–∞–Ω–∏–µ: {sku_data['description']}\n"
        if sku_data.get('category'):
            prompt += f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {sku_data['category']}\n"
        if sku_data.get('brand'):
            prompt += f"–ë—Ä–µ–Ω–¥: {sku_data['brand']}\n"

        # –ê—Ç—Ä–∏–±—É—Ç—ã —Ç–æ–≤–∞—Ä–∞
        if sku_attrs:
            prompt += "\n–•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò –¢–û–í–ê–†–ê:\n"
            for attr_name, attr_value in sorted(sku_attrs.items()):
                prompt += f"‚Ä¢ {attr_name}: {attr_value}\n"

        prompt += "\n–ö–ê–ù–î–ò–î–ê–¢–´ –ö–¢–†–£ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏):\n"

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø-5 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
        for i, entry in enumerate(similar_ktru_entries[:5], 1):
            payload = entry.payload
            score = getattr(entry, 'score', 0)
            ktru_attrs = self._extract_attributes(payload)
            attr_similarity = self._calculate_attribute_similarity(sku_attrs, ktru_attrs)

            prompt += f"\n{i}. –ö–æ–¥: {payload.get('ktru_code', '')}\n"
            prompt += f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {payload.get('title', '')}\n"
            prompt += f"   –°—Ö–æ–∂–µ—Å—Ç—å: —Ç–µ–∫—Å—Ç={score:.3f}, –∞—Ç—Ä–∏–±—É—Ç—ã={attr_similarity:.2f}\n"

            if ktru_attrs:
                prompt += "   –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ö–¢–†–£:\n"
                for attr_name, attr_value in sorted(ktru_attrs.items())[:5]:
                    prompt += f"   ‚Ä¢ {attr_name}: {attr_value}\n"

        prompt += """
–ê–õ–ì–û–†–ò–¢–ú –í–´–ë–û–†–ê:
1. –ù–∞–π–¥–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –≥–¥–µ –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç
2. –ü—Ä–æ–≤–µ—Ä—å —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç
3. –£–±–µ–¥–∏—Å—å —á—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
4. –ï—Å–ª–∏ –ø–æ–ª–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–µ—Ç - –∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω

–û–¢–í–ï–¢ (—Ç–æ–ª—å–∫–æ –∫–æ–¥ –∏–ª–∏ "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω"):"""

        return prompt

    def _validate_ktru_match(self, sku_data: Dict, ktru_data: Dict,
                             text_similarity: float, attr_similarity: float) -> Tuple[bool, float]:
        """–°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è SKU –∏ KTRU"""
        confidence = 0.0

        # –¢—Ä–µ–±—É–µ–º –≤—ã—Å–æ–∫—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
        if text_similarity >= 0.95:
            confidence += 0.6
        elif text_similarity >= 0.90:
            confidence += 0.4
        elif text_similarity >= 0.85:
            confidence += 0.2
        else:
            return False, 0.0

        # –ê—Ç—Ä–∏–±—É—Ç—ã –¥–æ–ª–∂–Ω—ã —Ö–æ—Ä–æ—à–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å
        if attr_similarity >= 0.8:
            confidence += 0.4
        elif attr_similarity >= 0.6:
            confidence += 0.2
        else:
            confidence *= 0.5  # –°–Ω–∏–∂–∞–µ–º –æ–±—â—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        sku_text = f"{sku_data.get('title', '')} {sku_data.get('description', '')}".lower()
        ktru_text = f"{ktru_data.get('title', '')} {ktru_data.get('description', '')}".lower()

        # –ò—â–µ–º –æ–±—â–∏–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ (–¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
        sku_words = set(w for w in sku_text.split() if len(w) > 3)
        ktru_words = set(w for w in ktru_text.split() if len(w) > 3)

        if sku_words and ktru_words:
            word_overlap = len(sku_words.intersection(ktru_words)) / min(len(sku_words), len(ktru_words))
            if word_overlap < 0.3:
                confidence *= 0.6

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if 'category' in sku_data and sku_data['category']:
            category_found = False
            for cat_key, keywords in self.category_keywords.items():
                if any(kw in sku_text for kw in keywords):
                    if any(kw in ktru_text for kw in keywords):
                        category_found = True
                        break

            if not category_found:
                confidence *= 0.7

        # –¢—Ä–µ–±—É–µ–º –æ—á–µ–Ω—å –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        is_valid = confidence >= 0.95

        return is_valid, confidence

    def classify_sku(self, sku_data: Dict, top_k: int = TOP_K) -> Dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è SKU –ø–æ –ö–¢–†–£ –∫–æ–¥—É —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {sku_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")

        if not self.qdrant_client:
            logger.error("‚ùå Qdrant –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ SKU
            sku_attrs = self._extract_attributes(sku_data)
            logger.info(f"üìã –ò–∑–≤–ª–µ—á–µ–Ω–æ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ SKU: {len(sku_attrs)}")

            # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
            sku_text = self._create_search_text(sku_data, sku_attrs)
            logger.info(f"üìù –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {sku_text[:200]}...")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            sku_embedding = generate_embedding(sku_text)
            logger.info(f"üî¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(sku_embedding)}")

            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ö–¢–†–£ –∫–æ–¥–æ–≤
            search_result = self.qdrant_client.search(
                collection_name=QDRANT_COLLECTION,
                query_vector=sku_embedding.tolist(),
                limit=top_k  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            )

            if not search_result:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –ö–¢–†–£ –∑–∞–ø–∏—Å–µ–π")
                return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏
            best_candidates = []

            for result in search_result:
                score = getattr(result, 'score', 0)
                payload = result.payload

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –Ω–∏–∑–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç—å—é
                if score < 0.8:
                    continue

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã KTRU
                ktru_attrs = self._extract_attributes(payload)

                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç–æ–≤
                attr_similarity = self._calculate_attribute_similarity(sku_attrs, ktru_attrs)

                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
                is_valid, confidence = self._validate_ktru_match(
                    sku_data, payload, score, attr_similarity
                )

                if is_valid:
                    best_candidates.append({
                        'result': result,
                        'confidence': confidence,
                        'text_similarity': score,
                        'attr_similarity': attr_similarity
                    })

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            best_candidates.sort(key=lambda x: x['confidence'], reverse=True)

            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç —Å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (>=0.98)
            if best_candidates and best_candidates[0]['confidence'] >= 0.98:
                best = best_candidates[0]
                payload = best['result'].payload
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {best['confidence']:.3f}")
                return {
                    "ktru_code": payload.get('ktru_code', '–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω'),
                    "ktru_title": payload.get('title', None),
                    "confidence": best['confidence']
                }

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Å—Ä–µ–¥–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            if self.llm and self.tokenizer and len(search_result) > 0:
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è LLM
                top_results = search_result[:10]

                prompt = self._create_classification_prompt(sku_data, top_results, sku_attrs)
                logger.info(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")

                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
                inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)
                inputs = {k: v.to(self.llm.device) for k, v in inputs.items()}

                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
                generation_config = GenerationConfig(
                    temperature=0.1,  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
                    top_p=0.9,
                    repetition_penalty=1.15,
                    max_new_tokens=50,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                with torch.no_grad():
                    generated_ids = self.llm.generate(
                        **inputs,
                        generation_config=generation_config
                    )

                # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
                response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                response = response[len(prompt):].strip()

                logger.info(f"ü§ñ –û—Ç–≤–µ—Ç LLM: '{response}'")

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞
                ktru_match = self.ktru_pattern.search(response)

                if ktru_match:
                    ktru_code = ktru_match.group(0)

                    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –∫–æ–¥–µ
                    for candidate in best_candidates:
                        if candidate['result'].payload.get('ktru_code') == ktru_code:
                            return {
                                "ktru_code": ktru_code,
                                "ktru_title": candidate['result'].payload.get('title', None),
                                "confidence": candidate['confidence']
                            }

                    # –ï—Å–ª–∏ –∫–æ–¥ –Ω–µ –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞—Ö, –∏—â–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
                    for result in search_result:
                        if result.payload.get('ktru_code') == ktru_code:
                            return {
                                "ktru_code": ktru_code,
                                "ktru_title": result.payload.get('title', None),
                                "confidence": 0.95
                            }

            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
            logger.info("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ö–¢–†–£")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ SKU: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {"ktru_code": "–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ktru_title": None, "confidence": 0.0}

    def _find_ktru_title_by_code(self, ktru_code: str, search_results: List) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è –ö–¢–†–£ –ø–æ –∫–æ–¥—É"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
            for entry in search_results:
                payload = entry.payload
                if payload.get('ktru_code') == ktru_code:
                    return payload.get('title', '')

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º –≤ –±–∞–∑–µ
            if self.qdrant_client:
                try:
                    scroll_result = self.qdrant_client.scroll(
                        collection_name=QDRANT_COLLECTION,
                        scroll_filter=Filter(
                            must=[
                                FieldCondition(
                                    key="ktru_code",
                                    match=MatchValue(value=ktru_code)
                                )
                            ]
                        ),
                        limit=1,
                        with_payload=True,
                        with_vectors=False
                    )

                    points, _ = scroll_result
                    if points:
                        return points[0].payload.get('title', '')
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –±–∞–∑–µ: {e}")

            return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ö–¢–†–£: {e}")
            return None


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
classifier = KtruClassifier()


def classify_sku(sku_data: Dict, top_k: int = TOP_K) -> Dict:
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ SKU"""
    return classifier.classify_sku(sku_data, top_k)